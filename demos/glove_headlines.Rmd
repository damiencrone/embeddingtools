---
title: "Annotating texts with GloVe"
output: html_notebook
author: "Damien Crone"
---
 
This document describes a test of implementing the word embedding-based measures to describe the semantic content of texts (essentially using a distributed dictionary approach; [Garten et al. 2018](http://doi.org/10.3758/s13428-017-0875-9)). Specifically, we use semantic representations of the Schwartz Values taxonomy of personal values, using a pre-trained word embedding model (GloVe), and validating the vector representations using a subsample of a corpus of news headlines.

# How to use this document

To make the document more readable (if you don't want to dig through code), in the top-right corner, you can click on _**Code**_, and select _**Hide All Code**_, and just limit the displayed content to any outputs and descriptive text.

To run the code, you will need:

1. The `embeddingtools` library, available from GitHub (code to download it is included below and commented out)
2. A pre-trained word embedding model (e.g., GloVe, available from available from https://nlp.stanford.edu/projects/glove/)
3. The validation dataset, *A Million News Headlines* (Kulkarni, 2017), available from https://www.kaggle.com/therohk/million-headlines/

# Loading a model

The `loadModel()` function loads a pre-trained word embedding model as a matrix with words as rows, and dimensions as columns. The specific model used here is a `r ncol(glove)`-dimensional GloVe model.

```{r}

# Load embeddingtools library
# install.packages("devtools")
# library(devtools)
# install_github("damiencrone/embeddingtools")
library(embeddingtools)

max_vocab_size = 250000

# Model obtained from http://nlp.stanford.edu/data/glove.42B.300d.zip
# Note to reduce resource demands, you may wish to use a model with lower
# dimensionality and/or reduce the vocabulary size.
glove = loadModel(
  file = "~/Desktop/glove_models/glove.6B.300d.txt",
  max_vocab_size = max_vocab_size
)

```

# Constructing a dictionary

Having loaded the word-embedding model, we can now construct vector representations of concepts of interest, using the `computeConceptRepresentation()` function. This can be done in one of two ways: First, as demonstrated here, one can create a list with one element for each concept in the dictionary, with each concept element containing a character vector of the words that will be used to define the concept. An assumption of the first approach is that each term is equally relevant to the concept. A second approach, allowing different terms to have different weights, is to input a table weights with terms in the rows and concepts in the columns. (A list provided as input will ultimately be converted to a table with all terms having equal weight.)

To create a concept representation, the `computeConceptRepresentation()` function simply searches through the vocabulary and creates a (weighted) average of the vectors for each term. In this example, the representation for *Power* will be the average of the vectors for the terms "power", "strength", and "achievement". In this example, the terms for each value concept are taken form a previously validated lexicon ([Bardi, Calogero & Mullen, 2008](http://doi.org/10.1037/0021-9010.93.3.483)).

As a simple descriptive exercise, we can print out the terms in the model vocabulary that are most similar to the concept representations. These will typically include some or all of the original terms in the dictionary, with the remaining terms being close semantic neighbours.

```{r}

schwartz_dictionary_list = list(
  `Power`          = c("power", "strength", "control"),
  `Achievement`    = c("achievement", "ambition", "success"),
  `Hedonism`       = c("luxury", "pleasure", "delight"),
  `Stimulation`    = c("excitement", "novelty", "thrill"),
  `Self direction` = c("independence", "freedom", "liberty"),
  `Universalism`   = c("unity", "justice", "equality"),
  `Benevolence`    = c("kindness", "charity", "mercy"),
  `Tradition`      = c("tradition", "custom", "respect"),
  `Conformity`     = c("restraint", "regard", "consideration"),
  `Security`       = c("security", "safety", "protection")
)

schwartz_vec = computeConceptRepresentation(
  model = glove,
  term_set = schwartz_dictionary_list
)

term_concept_similarity = computeTermConceptSimilarity(
  model = glove,
  concept_vec = schwartz_vec
)

top_list = findTopItems(term_concept_similarity, n = 7)
print(top_list)

```

# Validation data

To test the ability of the concept repesentations to describe the semantic content of texts, we computed the semantic similarity of each concept vector the the vector representations of texts in a corpus of over one million news headlines from the Australian Broadcasting Corporation ([Kulkarni, 2017](https://www.kaggle.com/therohk/million-headlines/)). For simplicity, we limit our analyses to a random sample of `r n_articles_to_sample` headlines.

```{r}

# Dataset obtained from https://www.kaggle.com/therohk/million-headlines/
news_dat = read.csv(
  file = "~/Desktop/glove_models/abcnews-date-text.csv",
  stringsAsFactors = FALSE
)

news_dat$word_count = stringr::str_count(
  string = news_dat$headline_text,
  pattern = "\\S+"
)

# Remove texts with fewer than 4 words
news_dat = news_dat[news_dat$word_count >= 4,]

# For simplicity, sample the validation dataset
set.seed(1234)
n_articles_to_sample = 10000
sample_ind = sample(x = 1:nrow(news_dat), size = n_articles_to_sample)
news_dat = news_dat[sample_ind,]

news_dat$headline_text_processed = removeWhiteSpace(
  convertTextToGloveFormat(news_dat$headline_text)
)

news_vec = computeTextVectors(
  text_vec = news_dat$headline_text_processed,
  model = glove
)

news_schwartz = computeTextConceptSimilarity(
  text_representation = news_vec,
  concept_vec = schwartz_vec
)
rownames(news_schwartz) = news_dat$headline_text

```

Below are the 5 article headlines with the highest semantic similarity to each personal value.

```{r}

top_headlines = findTopItems(concept_similarity = news_schwartz, n = 5)

cat(
  paste0(
    "Concept: ", names(top_headlines), "\n",
    lapply(top_headlines, function(x)paste("\t", names(x), collapse = "\n")),
    collapse = "\n"
  )
)

```

# References
Bardi, A., Calogero, R. M., & Mullen, B. (2008). A new archival approach to the study of values and value--behavior relations: Validation of the value lexicon. Journal of Applied Psychology, 93(3), 483–497. http://doi.org/10.1037/0021-9010.93.3.483

Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis. Behavior Research Methods, 50(1), 344–361. http://doi.org/10.3758/s13428-017-0875-9

Kulkarni, R. (2017). A Million News Headlines. http://doi.org/10.7910/DVN/SYBGZL. Retrieved from: https://www.kaggle.com/therohk/million-headlines/